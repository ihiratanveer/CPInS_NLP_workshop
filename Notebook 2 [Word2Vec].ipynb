{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iV2VefMMO0zgFTYdPKz3b2tD6bY6_7E6","timestamp":1671749246969},{"file_id":"1jxaRJoN9ND9hmrLsaPPMACbE547hr5_2","timestamp":1671641184126},{"file_id":"1qL8wDIfJTB0atlyWn_B4A6efbYG-p1w9","timestamp":1668590479573}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["### Loading Data  "],"metadata":{"id":"wswUyboqyR4s"}},{"cell_type":"code","source":["# Mount google drive to import data\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"EpPGeNMDrPxT","executionInfo":{"status":"ok","timestamp":1671794349640,"user_tz":-300,"elapsed":31079,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}},"outputId":"23ce58bf-c214-4d47-c34d-94e8236012bb","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# unzip the folder from drive to local space on colab\n","!unzip -q \"/content/gdrive/MyDrive/Workshop/nlp-getting-started.zip\" "],"metadata":{"id":"qnoz7ejtrl4w","executionInfo":{"status":"ok","timestamp":1671794354207,"user_tz":-300,"elapsed":4571,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["###Preprocessing"],"metadata":{"id":"DaGb0uzjv4jq"}},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","import string\n","from nltk.stem.porter import PorterStemmer\n","import numpy as np\n","\n","#Read data in to a pandas dataframe\n","df = pd.read_csv('/content/train.csv')\n","\n","#drop the columns which have Null values\n","df = df.dropna(how=\"any\", axis=1)\n","\n","# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\n","def clean_text(text):\n","    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","    and remove words containing numbers.'''\n","    #Lower Case\n","    text = str(text).lower()\n","\n","    #Remove links starting with https/www\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","\n","    #Remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","\n","    #Remove new line character\n","    text = re.sub('\\n', '', text)\n","\n","    #Remove words containing numbers\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","\n","    return text\n","\n","#load the english language stop words list\n","stop_words = stopwords.words('english')\n","\n","#We can also add more stopwords according to our data/problem\n","more_stopwords = ['u', 'im', 'c']\n","stop_words = stop_words + more_stopwords\n","\n","#Define a function to remove the stop words from the corpus\n","def remove_stopwords(text):\n","    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n","    return text\n","\n","#Initialize the Porter Stemmer Object\n","stemmer = PorterStemmer()\n","\n","#define the function to stem the words in the corpus\n","def stemm_text(text):\n","    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n","    return text\n","\n","def preprocess_data(text):\n","    # Clean puntuation, urls, and so on\n","    text = clean_text(text)\n","\n","    # Remove stopwords\n","    text = remove_stopwords(text)\n","\n","    # Stemm all the words in the sentence\n","    text = stemm_text(text)\n","    \n","    return text\n","\n","df['text_clean'] = df['text'].apply(preprocess_data)\n","df.head()"],"metadata":{"id":"I-pNBaunxZCW","colab":{"base_uri":"https://localhost:8080/","height":242},"executionInfo":{"status":"ok","timestamp":1671794357248,"user_tz":-300,"elapsed":3048,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}},"outputId":"61a1255e-f874-45c3-b20e-60780dd8a47a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["   id                                               text  target  \\\n","0   1  Our Deeds are the Reason of this #earthquake M...       1   \n","1   4             Forest fire near La Ronge Sask. Canada       1   \n","2   5  All residents asked to 'shelter in place' are ...       1   \n","3   6  13,000 people receive #wildfires evacuation or...       1   \n","4   7  Just got sent this photo from Ruby #Alaska as ...       1   \n","\n","                                          text_clean  \n","0          deed reason earthquak may allah forgiv us  \n","1               forest fire near la rong sask canada  \n","2  resid ask shelter place notifi offic evacu she...  \n","3       peopl receiv wildfir evacu order california   \n","4  got sent photo rubi alaska smoke wildfir pour ...  "],"text/html":["\n","  <div id=\"df-57a81bc6-1eca-4c9f-bbcb-321f0e833bad\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>deed reason earthquak may allah forgiv us</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la rong sask canada</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>resid ask shelter place notifi offic evacu she...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>peopl receiv wildfir evacu order california</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57a81bc6-1eca-4c9f-bbcb-321f0e833bad')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-57a81bc6-1eca-4c9f-bbcb-321f0e833bad button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-57a81bc6-1eca-4c9f-bbcb-321f0e833bad');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### Word2vec\n","Word2vec is vectorized representation of text. It is a great way to incorporate the context in the tasks related to the language modeling e.g. next word prediction, text similarity, recommendation system, chat bots etc.\n","\n","<img src='https://drive.google.com/uc?id=1AJ0ih-X4AOIlMsGjPZma8PlnjWh6-ek-' height='300px' width='600px' align='center'>\n","\n","Points to ponder upon:\n","\n","\n","*   Straight Red column\n","*   Blue Column\n","\n","This incorporates the understanding of semantics of human language into the neural network.\n","\n","**Another Example**<br>\n","Sentence 1: The **child** said he would grow up to be a doctor.\n","<br>\n","Sentence 2: The **kid** said he would grow up to be a doctor.\n","<br>\n","The embedding of kid and child would be similar in the vector space.\n","<br>\n","<br>\n","* The deep learning technique Word2vec can be acheived through 2 algorithms:\n","1. **CBOW**: Predicts the target word from the context.\n","2. **SkipGram**: Predicts the context using the target word.\n","\n","<br>\n","We can prepare the data using the aforementioned algorithms, train them using neural networks and get the embeddings. \n","\n","Other Models:\n","* GLOVE\n","* FASTEXT\n","\n","\n","\n"],"metadata":{"id":"KlwNdUm7abI2"}},{"cell_type":"code","source":["#Import gensim to load the word2vec model\n","import gensim\n","from gensim.models import Word2Vec"],"metadata":{"id":"-rrZ60nqafe0","executionInfo":{"status":"ok","timestamp":1671794358487,"user_tz":-300,"elapsed":1246,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Tokenize the preprocessed text\n","words_in_sentences=[]\n","for i in df['text_clean']:\n","    words_in_sentences.append(i.split())"],"metadata":{"id":"L23FQDnufNe1","executionInfo":{"status":"ok","timestamp":1671794359061,"user_tz":-300,"elapsed":577,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Actual tweet vs. tokenized tweet\n","\n","print(df['text_clean'][0])\n","print(words_in_sentences[0])"],"metadata":{"id":"z9ihFs5NxKw6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671794359062,"user_tz":-300,"elapsed":4,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}},"outputId":"91402b6d-0ef7-4355-9575-aa5f966c106b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["deed reason earthquak may allah forgiv us\n","['deed', 'reason', 'earthquak', 'may', 'allah', 'forgiv', 'us']\n"]}]},{"cell_type":"code","source":["# Train the word2vec model\n","# We are passing the tokenized tweets\n","# We have set the window size 5\n","# Minimum count =1\n","\n","w2v_model = gensim.models.Word2Vec(words_in_sentences,\n","                                   window=5,\n","                                   min_count=1)"],"metadata":{"id":"wk65_GyyAvvl","executionInfo":{"status":"ok","timestamp":1671794362028,"user_tz":-300,"elapsed":2969,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# it represents all of the words that our Word2Vec model learned a vector for. \n","# Or put another way, it's all of the words that appeared in the training data at least.\n","\n","# Total size of the vocabulary\n","print(len(w2v_model.wv.index2word))\n","\n","# let's print first 5 words of the vocabulary\n","print(w2v_model.wv.index2word[:10])"],"metadata":{"id":"QJRy-0QoBpir","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671794362028,"user_tz":-300,"elapsed":8,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}},"outputId":"f690f533-0c18-4cd1-9bf9-dff69e2f2267"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["13735\n","['like', 'fire', 'get', 'amp', 'bomb', 'new', 'via', 'one', 'peopl', 'go']\n"]}]},{"cell_type":"code","source":["# Generate embeddings for each word in the dataset\n","# Generate the sentences which now have the embeddings in place of the tokens\n","\n","embeddings = []\n","for sequence in words_in_sentences:\n","  embedding = []\n","  for word in sequence:\n","    vector = w2v_model.wv[word]\n","    embedding.append(vector)\n","  embeddings.append(embedding)"],"metadata":{"id":"KX9QD9EJ0PZg","executionInfo":{"status":"ok","timestamp":1671794364867,"user_tz":-300,"elapsed":2845,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#--------------TODO-------------#\n","\n","# check the length of the embedding vector and compare it to the total number of tweets\n","\n","#INSERT YOUR CODE HERE\n","\n","#check the length of first tweet and the learned embeddings\n","\n","#INSERT YOUR CODE HERE\n","\n","len(embeddings[0])\n","print(words_in_sentences[0])\n","\n"],"metadata":{"id":"8mTuFia61dLc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671794364867,"user_tz":-300,"elapsed":28,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}},"outputId":"28fb4ed2-ec48-45e7-f9a0-37ebddd4aa42"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['deed', 'reason', 'earthquak', 'may', 'allah', 'forgiv', 'us']\n"]}]},{"cell_type":"code","source":["#Vocabulary of learned word2vec\n","words=set(w2v_model.wv.index2word)"],"metadata":{"id":"SgF9X0yl96a3","executionInfo":{"status":"ok","timestamp":1671794364867,"user_tz":-300,"elapsed":26,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Similar words to token \"earthquak\" in our corpus\n","w2v_model.wv.most_similar('earthquak')"],"metadata":{"id":"92dWlnXM10Lv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671794364868,"user_tz":-300,"elapsed":27,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}},"outputId":"0e64d5bc-ac24-4392-8186-5871e3cf70b0"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('say', 0.9980059862136841),\n"," ('see', 0.9979246854782104),\n"," ('storm', 0.9978454113006592),\n"," ('amp', 0.9978289604187012),\n"," ('evacu', 0.9978169798851013),\n"," ('via', 0.9978048801422119),\n"," ('go', 0.9978014230728149),\n"," ('like', 0.9977470636367798),\n"," ('one', 0.9977437257766724),\n"," ('know', 0.9977187514305115)]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Similar words to token \"forest\" in our corpus\n","w2v_model.wv.most_similar('forest')"],"metadata":{"id":"hxpvQNKx4VX4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671794364868,"user_tz":-300,"elapsed":25,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}},"outputId":"5345875e-00ea-403c-a35f-ef810a7c0000"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('fire', 0.9981693029403687),\n"," ('emerg', 0.9981561303138733),\n"," ('amp', 0.9981023669242859),\n"," ('dont', 0.9980595707893372),\n"," ('make', 0.9980258345603943),\n"," ('say', 0.9980043172836304),\n"," ('flood', 0.998002827167511),\n"," ('time', 0.9979925751686096),\n"," ('go', 0.9979838728904724),\n"," ('get', 0.9979726076126099)]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["#-----------------TODO---------------#\n","\n","# Retrieve the similar words of \"deed\" and observe the difference in the similarity scores\n","\n","# INSERT YOUR CODE HERE"],"metadata":{"id":"NoWzc1OW4pQt","executionInfo":{"status":"ok","timestamp":1671794364868,"user_tz":-300,"elapsed":23,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Average Word2Vec for converting a given Sentence into numerical vector.\n","Note: Word2Vec is capable of providing an embedding for a given word but not for a sentence.\n","\n","Average Word2Vec is a technique in which the average of word embeddings of all the words given in a sentence is used as the numerical vector for a given sentence."],"metadata":{"id":"l01SOcHhnScD"}},{"cell_type":"markdown","source":["### **Reading Material**\n","\n","#### **Wrod2vec**\n","* https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained/notebook#5.-Vectorization\n","* https://github.com/krishnaik06/NLP-Live/blob/main/Day%205-%20NLP%20Word2vec%20And%20AvgWord2vec.ipynb\n","* https://github.com/dilipvaleti/Binary-Classification-using-word2vect/blob/main/Classification%20using%20word2vect.ipynb\n","\n"],"metadata":{"id":"r3J7QR1OvodJ"}},{"cell_type":"code","source":[],"metadata":{"id":"q8pIz8mWBtXr","executionInfo":{"status":"ok","timestamp":1671794364868,"user_tz":-300,"elapsed":23,"user":{"displayName":"Faiza Qamar","userId":"01955960500406931452"}}},"execution_count":14,"outputs":[]}]}